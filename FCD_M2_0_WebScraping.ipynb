{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPGI_UFRJ](imagens/ppgi-ufrj.png)\n",
    "# Fundamentos de Ciência de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[![DOI](https://zenodo.org/badge/335308405.svg)](https://zenodo.org/badge/latestdoi/335308405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PPGI/UFRJ 2020.3, 2022.2, 2024.2\n",
    "## Prof. Sergio Serra e Jorge Zavaleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Raspagem de dados com Python e BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A biblioteca BeautifulSoup é amplamente usada para analisar documentos HTML, XML ou outras linguagens de marcação  e extrair dados e gerar arquivos de dados mais claramente estruturados, isso é o que se chama de ***Web Scraping ou Raspagem de Dados***. \n",
    "\n",
    "![wst](imagens/web_scrap.png)\n",
    "\n",
    "> Em geral se usa essa técnica  para coletar informações de um website que não possuia API. Para compreenssão de alguns conceitos nessa aula é importante ter conhecimentos mínimos de HTML tree structure.\n",
    "\n",
    "\n",
    "### Leituras complementares\n",
    "> -  Básico de HTML- BeautifulSoap Aqui --> [Guia para iniciantes](https://www.vooo.pro/insights/guia-para-iniciantes-de-web-scraping-em-python-usando-beautifulsoup/)\n",
    "> -  Documentação oficial do BeautifulSoap --> [BeatifulSoap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das Bibliotecas Request e BeautifulSoap  \n",
    "\n",
    "A primeira coisa que precisamos fazer para examinar uma página da web é fazer o download dela. Podemos fazer o download de páginas usando a biblioteca **requests** do Python. \n",
    "\n",
    "A biblioteca requests fará uma solicitação GET para um servidor da web, que fará o download do conteúdo HTML de uma determinada página da Web. Existem vários tipos diferentes de solicitações que podemos fazer usando requests, GET é apenas uma.\n",
    "\n",
    "A biblioteca **BeautifulSoup** é usada para analisar o documento e extrair os textos a partir do elementos do HTML. Primeiro, precisamos importar a biblioteca e criar uma instância da classe BeautifulSoup para analisar um documento pretendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests       # Biblioteca Requests\n",
    "import bs4            # Biblioteca BeautifulSoap\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os próximos passos são testes de  outputs de uma requisição do site que desejamos \"raspar\" os dados. \n",
    "\n",
    "Utilizaremos  **http://dusp.mit.edu/people**  para criar um arquivo texto (CSV) com  dados livre de formatação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing output Requests\n",
    "response = requests.get('http://dusp.mit.edu/people')\n",
    "print(response.text)                         # Print o output da página HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando BeautifulSoup\n",
    "\n",
    "> #### Inspecionar um WebSite\n",
    ">> Os browsers já têm incorporado um \"inspecionador\", que é ativado ao clicar sobre uma página com botão direito do mouse.\n",
    "\n",
    "![Inspecionar](imagens/ins_chrome0.png)\n",
    "\n",
    "\n",
    "> #### Prettify \n",
    ">> Método do BS  para visualizar a estrutura separada da página HTML\n",
    "\n",
    ">Em linhas gerais o código a seguir irá analisar response.text criado pelo objeto BeautifulSoup e atribuido-o a soup. \n",
    "O argumento html.parser indica que queremos fazer a análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing output BS\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())                        # Print o output usando a função 'prettify' \n",
    "\n",
    "# Using the Beautiful Soup prettify() function,\n",
    "# we can print the page to see the code printed in a \n",
    "# readable and legible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navegando na estrutura de dados gerada pelo Prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the title \n",
    "# Access the title element\n",
    "#soup.titlelement\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the content of the title element\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data in the first 'p' tag\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data in the first 'a' tag\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "#soup.find_all('id')\n",
    "grouping = soup.find_all('div',class_='views-grouping')\n",
    "pprint(grouping[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve elements by class equal to link using the attributes argument\n",
    "people = soup.find_all('div', class_='views-row')\n",
    "pprint(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve elements by class equal to link using the attributes argument\n",
    "people_name = soup.find_all('a', class_='field-group-link card-link')\n",
    "pprint(people_name[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve name and lecturer\n",
    "pprint(people_name[0].get_text().split('\\n \\n\\n'))\n",
    "print('Comprimento:',len(people_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dados em Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maneira mais fácil de acessar elementos (dados) da página é gravá-los em um arquivo temporário e manipulá-los e salvá-los como objetos. \n",
    "\n",
    "Observe que os dados da página a ser raspada são organizados em \"counties\" e várias colunas com números. Logo, salvá-los em arrays é maneira mais lógica e fácil de trabalhar com os dados futuramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Load in loop\n",
    "data = []\n",
    "for i in range(0,len(people_name)):\n",
    "    p = people_name[i].get_text().strip().split('\\n \\n\\n') # data preparation\n",
    "    data.append(p)\n",
    "# data visualization\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "print(data[2])\n",
    "print(data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando e executando o arquivo de script do BeatutifuSoap (.py) para raspagem de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obeserve que podemos fazer a raspagem executando diretamente um script python através da linha de comandos, pois muitas vezes é um processo demorado. No caso dessa aula, dividimos o script em duas céluas para serem execuradas ao vivo. \n",
    "\n",
    "Caso deseje gerar um script .py, copie o conteúdo das duas próximas céluas em um único arquivo e execute diretamente no Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcionalmente pode ser executado dentro do Jupyter\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "# load and get the page from the website\n",
    "response = requests.get('http://dusp.mit.edu/people')\n",
    "\n",
    "# create the soup\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# find all the tags with class city or number\n",
    "#data = soup.findAll(attrs={'class':['menu','fips','tot-pop','median-income','no-housing-units','med-home-val','owner-occupied','house-w-debt','house-wo-debt']})\n",
    "p_name = soup.find_all('a', class_='field-group-link card-link')\n",
    "dados = []\n",
    "for i in range(0,len(p_name)):\n",
    "    p = p_name[i].get_text().strip().split('\\n \\n\\n') # data preparation\n",
    "    dados.append(p)\n",
    "# print 'data' to console optionally\n",
    "print('0',dados[0])\n",
    "print('1',dados[1])\n",
    "print('2',dados[2])\n",
    "print('3',dados[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = []\n",
    "y_activity = []\n",
    "v = []\n",
    "for j in range(0,len(dados)):\n",
    "    x_name.append(dados[j][0])\n",
    "    y_activity.append(dados[j][1])\n",
    "#\n",
    "print('name:',x_name[0])\n",
    "print('activity:',y_activity[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gravando dados em um arquivo texto (csv) usando um loop simples. A Lógica é mais ou menos a mesma em vários casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pseudo-código:\n",
    "\n",
    ">- 1 - Abra o arquivo de saida para escrever e apendar os dados raspados \n",
    ">- 2 - Defina os parâmetros do loop (While)\n",
    ">- 3 - Escreva os cabeçalhos (headers)\n",
    ">- 4 - Execute o Loop que irá produzir os elementos do array no arquivo \n",
    ">- 5 - Quando completar o Loop, fecha o arquivo \n",
    ">- 6 - Agora é só analisar os dados raspados :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcionalmente pode ser executado dentro do Jupyter\n",
    "import pandas as pd\n",
    "#\n",
    "data = {'name':list(x_name),'work':list(y_activity)}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "df.to_csv('data/people_data.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Não se esqueça de inserir seus métodos de Data Provenance na sua raspagem!\n",
    "\n",
    "Eles irão lhe ajudar a assegurar a rastreabilidade dos dados e do processo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outras Bibliotecas para fazer raspagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UrlLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Urllib** no Python 3 é uma coleção de módulos que se pode usar para trabalhar com URLs. A versão atual do urllib é composta dos seguintes módulos:\n",
    "\n",
    "- urllib.request\n",
    "- urllib.error\n",
    "- urllib.parse\n",
    "- urllib.rebotparser\n",
    "\n",
    "Para maiores informações - https://docs.python.org/3/library/urllib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![selenium](imagens/ws_sel.png)\n",
    "\n",
    "O conteúdo no qual se tem interesse nem sempre é de fácil acesso. Pode ser necessário interagir com o site para encontrá-lo: seja descendo uma página, clicando em menus e imagens. Ou talvez os dados desejados sejam apresentados utilizando JavaScript. Em todos esses casos, uma requisição do código fonte da página não é o bastante para encontrar as informações almejadas. \n",
    "\n",
    "O **Selenium** é usado nesses casos, é uma ferramenta que permite automatizar um browser (webdrivers), e realizar inúmeros tipos de ações com eles. Sua curva de aprendizado é mais íngrime, e por de fato abrir e controlar um browser, sua atuação é um pouco mais lenta. \n",
    "\n",
    "Por isso, seu uso não é recomendado quando os dados de interesse são de fácil acesso – a biblioteca *requests* dará conta do recado de forma mais rápida e eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "> Guia de instalação - [Instalação]('https://selenium-python.readthedocs.io/installation.html')\n",
    "\n",
    "> Instalar os drivers correspondentes ao browser usado.\n",
    "\n",
    "> Adicionar **PATH** dos drivers instalados ao path do sistema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para maiores informações - https://www.selenium.dev/\n",
    "\n",
    "Tente executar o script abaixo, foi adaptado de http://kevincsong.com/Scraping-stats.nba.com-with-python/\n",
    "\n",
    "OBS: Para executar o arquivo, se deve configurar o driver correspondente ao Browser utilizado, verificando a versão do browser utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.edge.options import Options\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Pegar conteúdo HTML a partir da URL do site com as estatísticas da NBA\n",
    "url = \"https://stats.nba.com/players/traditional/?PerMode=Totals&Season=2019-20&SeasonType=Regular%20Season&sort=PLAYER_NAME&dir=-1\"\n",
    "top10ranking = {}\n",
    "\n",
    "rankings = {\n",
    "    '3pontos': {'field': 'FG3M', 'label': '3PM'},\n",
    "    'pontos': {'field': 'PTS', 'label': 'PTS'},\n",
    "    'assistencias': {'field': 'AST', 'label': 'AST'},\n",
    "    'rebotes': {'field': 'REB', 'label': 'REB'},\n",
    "    'roubadas': {'field': 'STL', 'label': 'STL'},\n",
    "    'bloqueios': {'field': 'BLK', 'label': 'BLK'},\n",
    "}\n",
    "\n",
    "def buildrank(type):\n",
    "\n",
    "    field = rankings[type]['field']\n",
    "    label = rankings[type]['label']\n",
    "\n",
    "    edge_browser.find_element_by_xpath(\n",
    "        f\"//div[@class='nba-stat-table']//table//thead//tr//th[@data-field='{field}']\").click()\n",
    "\n",
    "    #Hack pra dar tempo pro click na janelinha de privacy anti-scraping  \n",
    "    time.sleep(3)      \n",
    "    \n",
    "    element = edge_browser.find_element_by_xpath(\n",
    "        \"//div[@class='nba-stat-table']//table\")\n",
    "    html_content = element.get_attribute('outerHTML')\n",
    "\n",
    "    # Parsear o conteúdo HTML - BeaultifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    table = soup.find(name='table')\n",
    "\n",
    "    # Estruturar conteúdo em um Data Frame - Pandas\n",
    "    df_full = pd.read_html(str(table))[0].head(50)\n",
    "    df = df_full[['Unnamed: 0', 'PLAYER', 'TEAM', label]]\n",
    "    df.columns = ['posicao', 'jogador', 'time', 'total']\n",
    "\n",
    "    # Transformar os Dados em um Dicionário de dados próprio\n",
    "    return df.to_dict('records')\n",
    "\n",
    "\n",
    "option = Options()\n",
    "#option.headless = True\n",
    "#!pip install webdriver-manager # install\n",
    "#option.binary_location = r'D:\\selenium\\msedgedriver.exe'\n",
    "\n",
    "# Path do geckodriver, necessário para funcionar, outra alyernativa é colocar o .exe no path do sistema\n",
    "#driver = webdriver.Chrome(executable_path=r'C:\\selenium_drivers\\chromedriver.exe')\n",
    "#driver = webdriver.Firefox(executable_path=r'C:\\selenium_drivers\\chromedriver.exe')\n",
    "#driver = webdriver.Edge(executable_path=r'D:\\selenium\\msedgedriver.exe')\n",
    "#driver = webdriver.Edge(options = option)\n",
    "#edge_driver = webdriver.Edge()\n",
    "# create object\n",
    "edge_browser = webdriver.Edge(r\"D:\\\\selenium\\\\msedgedriver.exe\")\n",
    "\n",
    "edge_browser.get(url)\n",
    "#\n",
    "edge_browser.implicitly_wait(10)      # aguarda 10 s entre os clicks no site da NBA\n",
    "\n",
    "for k in rankings:\n",
    "    top10ranking[k] = buildrank(k)\n",
    "\n",
    "edge_browser.quit()\n",
    "\n",
    "# Converter e salvar em um arquivo JSON\n",
    "with open('ranking.json', 'w', encoding='utf-8') as jp:\n",
    "    js = json.dumps(top10ranking, indent=4)\n",
    "    jp.write(js)\n",
    "    \n",
    "print(\"END GAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrapy](imagens/ws_ws.png)\n",
    "**Scrapy** é mais do que uma biblioteca, é um framework que permite não apenas extrair a informação de uma página, mas navegar por diversas páginas de forma sistemática e automatizada. \n",
    "\n",
    "Esse tipo de técnica recebe o nome de rastreador de rede, ou web crawler. Fornecendo uma lista de páginas inicias (seeds) que devem ser visitadas, a ferramenta é capaz de identificar todas as outras ligações destas páginas para outras – e então visitar essas novas páginas, e assim por diante. \n",
    "\n",
    "**Scrapy** é uma ferramenta poderosa quando se tem interesse em navegar um site, ou apenas uma parte dele ou por completo.\n",
    "\n",
    "Para maiores informações - https://scrapy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get(\"http://g1.globo.com\")\n",
    "page.status_code\n",
    "\n",
    "page.content\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "soup.title\n",
    "#soup.title.text\n",
    "\n",
    "#soup.findAll('a')\n",
    "#soup.find('a')\n",
    "\n",
    "#link_list = [l.get('href') for l in soup.findAll('a')]\n",
    "# print(link_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fudamentos para Ciência Dados &copy; Copyright 2020, 2022, 2024 - Sergio Serra & Jorge Zavaleta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
