{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPGI_UFRJ](imagens/ppgi-ufrj.png)\n",
    "# Fundamentos de Ciência de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[![DOI](https://zenodo.org/badge/335308405.svg)](https://zenodo.org/badge/latestdoi/335308405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PPGI/UFRJ 2020.3, 2022.2, 2024.2\n",
    "## Prof Sergio Serra e Jorge Zavaleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Módulo 2 - Limpeza e Preparação de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "A limpeza e preparação de dados é o processo de limpeza e transformação de dados sujos antes de passar para a fase de análise exploratória de dados. É a etapa mais importante no fluxo de trabalho da ciência de dados, que garante que os dados sejam limpos e preparados de forma a serem analisados ​​e modelados nas fases posteriores. \n",
    "\n",
    "Este módulo abordará a limpeza e preparação de dados e sua importância. A seguir, será abordado as etapas de limpeza de dados, entre as quais temos validação de dados, tratamento de valores ausentes, detecção de outliers e outros. Depois que as etapas de limpeza de dados forem explicadas, nos aprofundaremos nos tópicos de transformação de dados para preparação de dados, como padronização, normalização e algumas técnicas de engenharia de recursos. \n",
    "A seguir, conheceremos os tópicos de comparabilidade e consistência de dados. \n",
    "\n",
    "Exploraremos algumas bibliotecas e as usaremos para resolver casos de uso para identificar problemas da vida real e usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tópicos abordados: \n",
    "- Introdução à limpeza e preparação de dados \n",
    "- Importância da limpeza e preparação de dados \n",
    "- Identificação e correção de erros nos dados \n",
    "- Validação de dados Tratamento de valores ausentes \n",
    "- Tratamento de inconsistências e outliers \n",
    "- Transformação de dados \n",
    "- Criação de novas variáveis ​\n",
    "- ​Padronização \n",
    "- Normalização \n",
    "- Técnicas de engenharia de recursos\n",
    "- Integração de dados, concatenação, merge e join \n",
    "- Bibliotecas usadas para limpeza de dados \n",
    "- Comparabilidade de dados e consistência de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução à limpeza e preparação de dados \n",
    "A limpeza e preparação de dados são as etapas fundamentais no processo de análise de dados. \n",
    "\n",
    "Os dados brutos contêm principalmente erros, inconsistências, faltas e outros problemas que podem criar problemas relativos à análise precisa. A limpeza de dados identifica e corrige esses erros, enquanto a preparação de dados se concentra em transformar os dados em um formato conveniente para análise. \n",
    "\n",
    "Os dados finais preparados devem ser precisos, completos e consistentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância da limpeza e preparação de dados \n",
    "A limpeza e preparação de dados são cruciais para uma análise confiável e significativa. Dados de má qualidade podem levar a resultados tendenciosos ou incorretos, afetando a integridade geral e a validade da análise. \n",
    "\n",
    "A limpeza e a preparação dos dados ajudam a eliminar erros, inconsistências e discrepâncias que podem surgir da coleta de dados ou dos processos de armazenamento. Também envolve o tratamento de valores ausentes, garantindo que os dados estejam corretos e resolvam quaisquer problemas de qualidade dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificação e correção de erros nos dados \n",
    "Identificar e corrigir erros nos dados é uma etapa crucial quando estamos limpando os dados, o que ajuda a melhorar a qualidade dos datasets, eliminando duplicatas ou valores incorretos. \n",
    "\n",
    "Abordaremos a validação de dados, manipulação de valores ausentes, exploramos as técnicas de identificação e substituição de valores ausentes ou remoção completa de valores ausentes, se necessário.\n",
    "\n",
    "Também examinaremos várias técnicas de detecção de valores discrepantes e as experimentaremos nos datasets dos exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação de dados\n",
    "A validação de dados consiste em examinar os dados brutos para identificar erros, inconsistências e ausências, garantindo que eles atendam a determinadas regras ou restrições.\n",
    "\n",
    "Técnicas comuns predefinidas usadas para validação de dados incluem verificações de intervalo, verificações de formato e verificações de consistência. \n",
    "\n",
    "Por exemplo, verificar se os dados numéricos estão dentro dos intervalos esperados ou verificar se as datas estão no formato correto ou se os tipos de dados no dataset estão no formato correto (por exemplo, string, float, inteiro, booleano e assim por diante) ou verificar se há valores ausentes no dataset ou linhas duplicadas. \n",
    "\n",
    "Esses são alguns dos testes de validação de dados que devemos realizar para detectar possíveis erros no conjunto de dados e assegurar a precisão das informações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de valores ausentes \n",
    "Os valores ausentes são um problema comum em datasets do mundo real. O tratamento de valores ausentes é essencial para evitar distorções ou incompletudes. \n",
    "\n",
    "Várias abordagens podem ser usadas para lidar com valores ausentes, como exclusão e imputação, ou técnicas avançadas como imputação múltipla também podem ser usadas. \n",
    "\n",
    "A exclusão envolve a remoção de linhas ou colunas com valores faltantes, enquanto a imputação envolve a estimativa de valores faltantes com base em padrões. A escolha da abordagem depende da natureza dos dados faltantes e dos objetivos da análise. \n",
    "\n",
    "Para detectar e lidar com valores ausentes, podemos usar algumas das técnicas a seguir. \n",
    "\n",
    "O conjunto de dados é o conjunto de dados de partidas do IPL de 2008. \n",
    "O conjunto de dados tem detalhes de cada partida do IPL, começando do local até a temporada, do número da partida ao nome do time, da decisão do lançamento ao time vencedor, da margem ao jogador da partida.\n",
    "\n",
    "O quadro de dados tem 950 linhas e 20 O código a seguir é fornecido para demonstração. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librarys import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CVS file into a pandas DataFrame\n",
    "data_pu2020 = pd.read_csv('data/cursos-prouni2020.csv', delimiter=',')\n",
    "# Checking the first few rows of the DataFrame\n",
    "data_pu2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando as dimesões do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimension\n",
    "data_pu2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando o número de valores ausentes usando o seguinte código: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando valores ausentes \n",
    "print(data_pu2020.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *isnull()* é uma função que retorna Verdadeiro/Falso, haja ou não um valor nulo;\n",
    "- *sum()* é uma função que retorna o número total de valores Verdadeiros.\n",
    "\n",
    "O resultado do código anterior apresenta o *número de valores faltantes para cada variável (a variável do método tem valores faltantes máximos)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para preencher os dados faltantes usaremos a técnica de *imputação de dados* para imputar os valores faltantes da variável *bolsa_integral_cotas*.\n",
    "\n",
    "Podemos imputar usando a média ou mediana. \n",
    "- A **média** é usada em cenários quando a distribuição é uniforme (não tão distorcida ou assimetrica) ou não há valores discrepantes (*outliers*). \n",
    "- A **mediana** é usada quando a distribuição é assimétrica. \n",
    "\n",
    "Verificar se a assimetria de alguma variável do dataset é distorcida usando a a função de assimetria (skew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The histogram of the bolsa_integral_cotas variable\n",
    "sns.histplot(data_pu2020['bolsa_integral_ampla']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O histograma da variável *bolsa_integral_ampla* mostra claramente que a distribuição é distorcida para a direita, uma vez que os dados são distribuídos principalmente à esquerda e a cauda está à direita. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando a assimetria da variável usando a função de assimetria (skew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness (assimetria) function\n",
    "assimetria = data_pu2020['bolsa_integral_ampla'].skew() \n",
    "print('Valor da assimetria:', assimetria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As condições para a assimetria são as seguintes:\n",
    "- Assimetria > 0: Distribuição distorcida à direita \n",
    "- Assimetria < 0: Distribuição distorcida à esquerda \n",
    "- Assimetria próxima de 0: Distribuição aproximadamente simétrica "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor da assimetria da variável *bolsa_integral_ampla* é 5.0838\n",
    "\n",
    "A assimetria é > 0, portanto está assimetrica à direita e em sincronia com o histograma. Logo, imputaremos valores usando a *mediana*. \n",
    "\n",
    "Agora vamos imputar o valor ausente à mediana da variável bolsa_integral_ampla. O código é o seguinte:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencha os valores faltantes com a mediana (Imputação) \n",
    "data_pu2020['bolsa_integral_ampla'] = data_pu2020['bolsa_integral_ampla'].fillna(data_pu2020['bolsa_integral_ampla'].median())\n",
    "#Impute with median\n",
    "data_pu2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O os valores ausentes são substituídos pela mediana da variável *bolsa_integral_ampla*. \n",
    "A função *fillna()* é usada para realizar a operação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores ausentes são substituídos na variável *bolsa_integral_ampla* pela média arimética dos valores da variável. \n",
    "\n",
    "**Observação**\n",
    "> As variáveis podem ser descartadas ou não, isso vai depender do tipo de análise a ser feito com o dataset que depende dos tipos de dados. Dados importantes podem ser perdidos ao excluir ou imputar dados usando média e mediana. Os dados de contextos diferentes devem ser tratados com cuidado.\n",
    "\n",
    "Por exemplo: De forma geral, não se pode usar média ou mediana para imputar dados médicos, pois as variáveis dependem umas das outras ou estão relacionadas entre elas nas diferentes doenças no contexto médico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem ser excluidas todas as linhas se houver mais valores ausentes, como a variável do método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing value\n",
    "data_drop = data_pu2020.dropna(subset=['bolsa_parcial_cotas'])\n",
    "data_drop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método *dropna()* excluirá todas as linhas que tenham pelo menos uma célula faltando (valores 'NaN')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de inconsistências e outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erros de entrada de dados, erros de medição e falhas de sistema são alguns exemplos que podem causar o aparecimento de dados inconsistentes ou incorretos. Normalmente, valores discrepantes numéricos são aqueles que se desviam da média em mais de três padrões. \n",
    "\n",
    "Manter a integridade dos dados requer detectar e gerenciar anomalias e discrepâncias. Técnicas como perfil de dados, visualização de dados e métodos estatísticos podem ser empregados para detectar inconsistências e uma delas é a identificação visual onde plotamos os dados e simplesmente procuramos os pontos de dados que são diferentes do resto dos dados, por exemplo, usando box plots, podemos identificar os outliers. \n",
    "\n",
    "Os box plots são usados ​​para descobrir como os dados são distribuídos do mínimo ao máximo. Também nos fornece informações sobre a mediana (50º percentil), 25º percentil e 75º.\n",
    "\n",
    "O gráfico mostra que estamos usando a biblioteca *Seaborn* para usar o boxplot. A variável *bolsa_integral_ampla* está sendo usada neste caso para mostrar no boxplot e identificar os outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using box plot \n",
    "# plot de box plot\n",
    "sns.boxplot (data_pu2020['bolsa_integral_ampla']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura do Boxplot mostra o intervalo de valores de *bolsa_integral_ampla* no dataset de bolsas concedidas pelo PROUNI em 2020.\n",
    "\n",
    "Na Figura pode-se observar que a maioria dos resultados de *bolsa_integral_ampla* variam de 0 a 5, onde 5 é o valor máximo e 0 é o valor mínimo. 75% dos dados estão abaixo de 5 (valor aproximado) e 25% dos dados estão abaixo de 2 (valor aproximado). \n",
    "\n",
    "Observarmos que há números variando de 5 a 65. Os números maiores que 40 são discrepantes, pois não se correlacionam com o intervalo mínimo a máximo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica Inter-Quartile Range (IQR) \n",
    "Esta técnica é usada com frequência para detectar e lidar com *outliers*. Esta técnica identifica outliers ao comparar os pontos dos dados. O IQR é a diferença entre os percentis 75 e 25 de uma distribuição. \n",
    "\n",
    "Outliers são normalmente definidos como pontos de dados menores que o **25º percentil - 1,5*IQR** ou maiores que o **75º percentil + 1,5*IQR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecção de outlier \n",
    "# Técnica IQR \n",
    "# Trace a distribuição da coluna 'bolsa_integral_ampla' \n",
    "plt.hist(data_pu2020['bolsa_integral_ampla']) \n",
    "plt.xlabel('bolsa_integral_ampla') \n",
    "plt.ylabel('Frequencia') \n",
    "plt.title('Distribuição dos valores de bolsa_integral_ampla') \n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando outliers usando o intervalo interquartil (IQR) \n",
    "Q1 = data_pu2020[ 'bolsa_integral_ampla'].quantile(0.25) \n",
    "Q3 = data_pu2020['bolsa_integral_ampla'].quantile(0.75) \n",
    "IQR = Q3 - Q1 \n",
    "print('IQR=',IQR)\n",
    "# Defina outliers como valores fora do intervalo de 1,5 IQR \n",
    "outliers = data_pu2020[data_pu2020['bolsa_integral_ampla'] < Q1 - 1.5*IQR] \n",
    "#concatenation\n",
    "outliers = pd.concat([outliers,(data_pu2020[data_pu2020['bolsa_integral_ampla'] > Q3 + 1.5*IQR])])\n",
    "# print outliers\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código anterior calcula o intervalo interquartil usando o primeiro e o 3º Quartil (o 25º e o 75º percentil). Então os outliers são calculados com base em **Q3+1,5*IQR**. Aqui Q1 é o 25º percentil e Q3 é o 75º.\n",
    "\n",
    "Também temos um histograma traçado que nos dá uma ideia da distribuição. A maioria dos dados está concentrada entre 0 e 5. Os valores discrepantes ficam após 40–60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the z-scores\n",
    "z_scores = np.abs(data_pu2020['bolsa_integral_ampla'] - data_pu2020['bolsa_integral_ampla'].mean()) / data_pu2020['bolsa_integral_ampla'].std()\n",
    "# Define outliers as data points that are more than 3 standard deviations away from the mean\n",
    "outliers = z_scores[z_scores > 3]\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado do código acima mostra a detecção de outliers usando a técnica z-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A técnica de *z-score* no código anterior usa a fórmula estatística $\\frac{(x-μ)}{\\sigma}$. Onde, $\\sigma$ é o desvio padrão, $μ$ é a média e $x$ é um dado. \n",
    "\n",
    "O resultado do código acima mostra o conjunto de pontos de dados que são os outliers detectados usando $z-score$. A primeira coluna é o número do índice do quadro de dados e a 2ª coluna nos dá o número que tem um $valor> 3$ para detectar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir é apresentado os outliers com índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de outliers\n",
    "out_index = data_pu2020.iloc[list(outliers.index)]\n",
    "out_index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado anterior mostra os dados com os índices que contém os outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem também outros de detecção de valores faltantes, como os métodos de detecção de valores discrepantes de aprendizado de máquina, [*Isolation Forest*](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) e o *local outlier Factor (LOF)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação de dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformação de Dados é usada para alterar a escala ou distribuição das variáveis ​​ou converter um tipo de dados em outro. As variáveis ​​numéricas podem ser transformadas para reduzir a assimetria, tomando-se uma variável categórica que pode ser convertida em uma *dummy*. \n",
    "\n",
    "Novas variáveis ​​também podem ser criadas por conveniência, quando necessário. \n",
    "\n",
    "As técnicas de padronização e normalização também são utilizadas para transformação de dados para convertê-los em uma escala adequada para processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de novas variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​​A criação de novas variáveis ​​é necessária para compreender bem os dados e introduzir novas funções a partir das existentes. As novas variáveis ajudam na solução de problemas relacionados à limpeza e análise de dados. \n",
    "\n",
    "O código de exemplo a seguir nos ajuda a entender a criação de novas variáveis. O dataset contém as seguintes variaveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pu2020.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pu2020['curso'] = data_pu2020['curso_busca'].apply(lambda x: x*2)\n",
    "data_pu2020['Bolsa_integral'] = data_pu2020['bolsa_integral_cotas'].apply(lambda x: x*2)\n",
    "data_pu2020['Bolsa_integral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima cria uma nova coluna elevando ao quadrado a variável de *bolsa_integral_cotas* e criando uma nova variável conhecida como *bolsa_integral* usando a função $lambda$ dentro do quadro de dados e depois usando a função *apply()*. \n",
    "O novo DataFrame de dados possui a variável *bolsa_integral*, conforme mostrado a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pu2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos usar a técnica de *binning* (quando usar? binning = k-means?) para categorizar os turnos em diferentes categorias, como:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um novo recurso chamado 'BolsaGroup' \n",
    "data_pu2020['BolsaGroup'] = data_pu2020['Bolsa_integral'].apply(lambda x: 'Ruim' if x < 30 else 'Média' if x < 50 else 'Boa')\n",
    "data_pu2020.head()\n",
    "# O quadro de dados recém-criado é o seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe uma técnica chamada *hot encoding* que é aplicada às variáveis ​​categóricas para convertê-las em *dummy*. Cada categoria tem sua variável *dummy*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot codifica a coluna 'turno' \n",
    "data_he = pd.get_dummies(data_pu2020, columns=['turno'], dtype = int) \n",
    "data_he.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o numero de colunas aumentou\n",
    "data_he.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função *get_dummies()* é usada para converter em variáveis ​​​​fictícias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável *dummy* representada para a variável *turno*. A variável *turno* é uma variável categórica {Integral, Matutino, Noturno, Vespertino}. O dataframe de dados de saída anterior mostra claramente que as novas variáveis {turno_Integral, turno_Matutino, turno_Noturno, turno_Vespertino} como 1 (sempre que o estudante pertença a esse turno) e como 0 (sempre que o estudante não peretença a esse turno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ​Padronização "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A padronização é uma técnica para transformação de dados originais em dados com média $0$ e desvio padrão igual $1$. Isso acontece subtraindo o valor dos dados do ponto de dados e depois dividindo-o pelo padrão. \n",
    "\n",
    "A fórmula é $\\frac{z-\\mu}{\\sigma}$, onde $\\mu$ é a média das amostras e $\\sigma$ é o desvio padrão das amostras.\n",
    "\n",
    "A técnica é útil quando o os dados seguem a distribuição normal. \n",
    "\n",
    "O código a seguir demonstra o conceito. A padronização é feita através da variável bolsa_integral_cotas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padronizar a série \n",
    "padronizado = (data_pu2020['bolsa_integral_cotas'] - data_pu2020['bolsa_integral_cotas'].mean()) / data_pu2020['bolsa_integral_cotas'].std() \n",
    "print('O formato de dados padronizado é o seguinte:')\n",
    "print(padronizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ​Normalização "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um procedimento de normalização efetua uma transformação na escala de avaliação, que em geral passa a utilizar um intervalo (0, 1). Ou seja, o elemento de menor preferência tem valor $O$ e o de maior preferência tem valor $1$. No entanto, no contexto de avaliação preferencial, existem procedimentos que, embora chamados de normalização, passam a utilizar escalas diferentes de $(0,1)$.\n",
    "\n",
    "A normalização dimensiona os dados para um intervalo fixo (normalmente de 0). Isso é feito subtraindo o valor mínimo dos dados e depois dividindo o intervalo. \n",
    "\n",
    "A fórmula é $(x-xmin)/(xmax-xmin)$, onde $xmax$ e $xmin$ são os valores máximo e mínimo do dataset. \n",
    "\n",
    "É melhor trabalhar com a normalização quando os dados não contêm valores discrepantes, pois eles podem distorcer os valores dimensionados. \n",
    "\n",
    "O código a seguir normaliza a variável bolsa_integral_cotas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizada = (data_pu2020['bolsa_integral_cotas']-data_pu2020['bolsa_integral_cotas'].min()) /(data_pu2020['bolsa_integral_cotas'].max() - data_pu2020['bolsa_integral_cotas'].min()) \n",
    "print('A série pandas normalizada da variável bolsa_integral_cotas:')\n",
    "print(normalizada) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarização vs Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribuição dos dados é alterada tendo a média em 0 e o desvio padrão em 1. A padronização é útil quando a distribuição cumpre este requisito. \n",
    "\n",
    "No entanto, nem sempre é obrigatória. Além disso, ao contrário da normalização, a padronização não tem um intervalo delimitador e, portanto, não é afetada. \n",
    "\n",
    "Em contraste, a normalização muda o intervalo dos dados para que fique entre 0 e 1 ou -1. A distribuição dos dados neste caso é desconhecida. \n",
    "\n",
    "Os valores discrepantes afetam isso devido a limites de intervalo como $[0,1]$ ou $[-1,+1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de engenharia de recursos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem algumas técnicas de engenharia de recursos, como transformação logarítmica, transformação de raiz quadrada e polinômio. Essas técnicas são detalhadas a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação Logarítmica (Log Transform)\n",
    "\n",
    "A transformação logarítmica é usada para lidar com dados distorcidos. Ajuda a lidar com valores grandes, reduzir a assimetria e transformar os dados em um formato mais normal ou semelhante ao Gaussiano. \n",
    "\n",
    "O código a seguir mostra que a variável *mensaliddae* do DataFrame está correta, ou seja, a maioria dos dados está concentrada no lado esquerdo e, portanto, precisa de transformação logarítmica. \n",
    "\n",
    "A função *np.log()* é usada para realizar a transformação logarítmica. É apresentado o Histogramas da mensalidade original e logo a transformação logarítmica da mesma variável.\n",
    "\n",
    "O parâmetro edgecolor é definido como 'vermelho' para uma melhor visualização do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "# Execute a transformação do log \n",
    "data_log = np.log(data_drop['mensalidade']) \n",
    "# Defina uma grade de gráficos \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2) \n",
    "# Crie histogramas \n",
    "axs[0].hist(data_drop['mensalidade'], edgecolor='red') \n",
    "axs[0].set_xlabel('Mensalidade')\n",
    "axs[1].hist(data_log, edgecolor='red')\n",
    "axs[1].set_xlabel('Mensalidade - LT')\n",
    "# Adicione um título a cada histograma \n",
    "axs[0].set_title('Mensalidade Original') \n",
    "axs[1].set_title('Mensalidade - Log-Transformed') \n",
    "# espaço entre os gráficos\n",
    "plt.subplots_adjust(wspace=0.5,hspace=0.1)\n",
    "# Exibe os gráficos \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gráfico anterior mostra os dados distorcidos à direita, onde a maioria dos dados está concentrada à esquerda, e à direita, temos a versão da transformada logarítmica, que é a distribuição normal dos dados distorcidos anteriores.\n",
    "\n",
    "Observar que ainda existem valores que devem ser verificados. Outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação da raiz quadrada (SRT)\n",
    "\n",
    "A transformação de raiz quadrada é um método de transformação de dados usado para reduzir a distorção dos dados. Usado especificamente quando os dados estão um pouco distorcidos para a direita. \n",
    "\n",
    "Uma transformação de raiz quadrada envolve obter a raiz quadrada de cada ponto de dados no dataset. Isso ajuda a reduzir a variação e torna os dados distribuídos de maneira mais normal. \n",
    "\n",
    "O código a seguir mostra a operação de transformação de raiz quadrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square Root Transform (SRT)\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "# Execute a transformação da raiz quadrada \n",
    "data_sqrt = np.sqrt(data_pu2020['mensalidade']) \n",
    "# Defina uma grade de gráficos \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2 ) \n",
    "# Cria histogramas \n",
    "axs[0].hist(data_pu2020['mensalidade'], edgecolor='red')\n",
    "axs[0].set_xlabel('Mensalidade') \n",
    "axs[1].hist(data_sqrt, edgecolor='red') \n",
    "axs[1].set_xlabel('Mensalidade - SRT')\n",
    "# Adiciona um título a cada histograma \n",
    "axs[0] .set_title('Mensalidade Original') \n",
    "axs[1].set_title('Mensalidade - SRT') \n",
    "# espaço entre os gráficos\n",
    "plt.subplots_adjust(wspace=0.5,hspace=0.1)\n",
    "# Exibe os gráficos \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa no gráfico acima, os dados originais são distorcidos à direita e os dados transformados pela raiz quadrada também são moderadamente distorcidos à direita, mas estão próximos do normal. \n",
    "\n",
    "Não é tão eficiente quanto a versão de transformação logarítmica, pois os dados originais são distorcidos à direita e não à esquerda. Porém, para entender o conceito, é bom usar a função *sqrt()* neste caso com os dados disponíveis. \n",
    "\n",
    "Normalmente, se compararmos, a transformação logarítmica se sai melhor do que a transformação raiz quadrada. A transformação logarítmica pode compactar valores altos melhor do que a transformação raiz quadrada. No entanto, a raiz quadrada só pode ser usada para valores > = 0, enquanto o logarítmo não pode ser usado quando os valores são iguais a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integração de dados, concatenação, merge e join \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integração de dados \n",
    "\n",
    "A integração de dados combina dados de várias fontes para ter uma visão composta do conjunto de dados. Muitas vezes, isso é necessário durante a análise de dados, pois ajuda a trabalhar com o dataset completo. \n",
    "\n",
    "Várias operações são usadas com a função *concat* para realizar integração de dados usando pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenação \n",
    "\n",
    "A função *concat* do pandas é usada para concatenar linhas de um dataframe ao final de outro dataframe, retornando assim um novo objeto. Se houver colunas ou linhas ausentes em qualquer dataframe, as células serão substituídas por NaN. \n",
    "\n",
    "O código a seguir apresenta o conceito de concatenação de 2 dataframes. \n",
    "\n",
    "Aqui, há 3 linhas em cada datafram e os índices também são definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenation\n",
    "import pandas as pd \n",
    "df1 = pd.DataFrame({'Nome':['João', 'João', 'Pedro' , 'Caio'], \n",
    "                    'Telefone': ['12121', '343434', '565656', '787878'], \n",
    "                    'Carros': ['azul', 'preto', 'verde' , 'amarelo']})\n",
    "df2 = pd.DataFrame({'Nome':['João', 'João', 'Pedro' , 'Caio'], \n",
    "                    'Telefone': ['12121', '343434', '565656', '787878'], \n",
    "                    'Carros': ['BRANCO', 'PRATA', 'PRATA' , 'VERMELHO']})\n",
    "# concat\n",
    "df1_df2 = pd.concat([df1, df2]) \n",
    "df1_df2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados resultantes quadro mostra que o segundo dataframe é mantido abaixo do primeiro dataframe. E os índices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir apresenta o resultado de uma concatenação com valores faltantes (NaN) de algumas colunas. É um problema de *missing data*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'Nome':['João', 'Marcelo', 'Thiago' , 'Caio'], \n",
    "                     'Irmãos': ['1', '3', '2' , '2']}, index=[4,5,6,7])\n",
    "df1_df3 = pd. concat([df1,df3])\n",
    "df1_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função *concat* também pode ser usada para concatenar dataframes ao longo do eixos das colunas (fila: axis = 0 ou coluna: axis = 1). Adicona colunas ou filas segundo o valor de axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando os dataframes ao longo do eixo da coluna (axis=1)\n",
    "con_cols = pd.concat([df1, df2], axis=1)\n",
    "print('Adiciona colunas à direita')\n",
    "con_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando os dataframes ao longo do eixo das filas (axis=0)\n",
    "con_filas = pd.concat([df1, df2], axis=0)\n",
    "print('Adiciona filas na parte inferior do dataframe')\n",
    "con_filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "A operação de mesclagem (merge) no pandas combina dataframes em um dataframe unindo linhas usando uma ou mais chaves. É semelhante à operação JOIN em SQL.\n",
    "\n",
    " O código a seguir explica a função merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DataFrames \n",
    "merge_nome = pd.merge(df1, df3, on='Nome') \n",
    "merge_nome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código possui 2 dataframes df1 e df2 com o nome dos proprietários de carros. A operação *merge* é usada para combinar ambos os dataframes na coluna *Nome* como é usado em JOIN em SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "merge_nomes = pd.merge(df1, df2, on='Nome') \n",
    "merge_nomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*nome de colunas ._x ou ._y*?\n",
    "\n",
    "A chave para o merge foi 'Nome', todas as outras colunas iguais entre as tabelas são separadas em _x e _y, onde:\n",
    "- _x Corresponde aos valores que existiam na tabela da esquerda (df1)\n",
    "- _y Corresponde aos valores que existiam na tabela da direita (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "A função .join() é usada para combinar frames de dados com base em seus índices. É muito semelhante à função *merge()* mencionada anteriormente, mas usa os índices para unir os dataframes, ao contrário das colunas da função *merge()*\n",
    "\n",
    "Ela faz a junção à esquerda como acima e se não houver correspondência em determinadas linhas, o resultado é apresentado a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jleft = pd.DataFrame({'A': ['A0', 'A1', 'A2']}, index=['x', 'y', 'z']) \n",
    "jright = pd.DataFrame({'B': ['B0', 'B1', 'B2']}, index=['x', 'a', 'b']) \n",
    "joins = jleft.join(jright) \n",
    "joins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código anterior usa a função *join()* e os dataframes jleft e jright são unidos com base nos índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras bibliotecas usadas para limpeza de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de limpeza e preparação de dados é feito por bibliotecas Python tradicionais (R támbem tem bibliotecas para Limpeza de Dados), como *pandas*, *numpy* e assim por diante, mas existem outras bibliotecas também, como *polars*, que são alternativas muito interessantes para as bibliotecas anteriores e são listadas a seguir:\n",
    "- **Pyjanitor** - Pyjanitor é uma biblioteca Python de código aberto construída sobre o pacote Janitor em R. Ela faz o processo de limpeza com a ajuda do método de encadeamento, onde remove, renomeia, exclui colunas, remove valores ausentes e assim por diante. É muito semelhante ao operador pipe usado em R. Maiores detalhes de utilização em [Pyjanitor](https://pyjanitor-devs.github.io/pyjanitor/)\n",
    "- **Ftfy** - A biblioteca ftfy é chamada de corrige texto e ajuda a corrigir problemas relacionados à codificação e decodificação Unicode. É muito comum em arquivos .xlsx ou .csv onde alguns textos são convertidos em textos irrelevantes. A biblioteca ftfy for usada em textos para fornecer textos limpos. Maiores detalhes em [Ftfy](https://pypi.org/project/ftfy/)\n",
    "- **Polars** - Polars é a biblioteca alternativa do pandas usada para processar dados mais rapidamente do que a biblioteca pandas convencional. Polars não usa um índice para o dataframe em comparação com pandas, ele suporta mais operações paralelas do que pandas, além disso, *Polars* suporta avaliação lenta (lazy) que ajuda no uso eficiente da memória, examinando as consultas e otimizando-as. Em comparação, os pandas usam a técnica de avaliação ansiosa (eager), que avalia uma consulta sem examiná-la ou otimizá-la, aumentando assim o uso de memória. Maiores detalhes em [Polars](https://pola.rs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assegurar consistência e comparabilidade de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados uniformes e precisos em vários pontos de acesso ou instâncias podem ser considerados dados consistentes. A consistência de dados refere-se à uniformidade e precisão dos dados em diferentes sistemas e ao longo do tempo. Isso significa que os dados devem ser os mesmos, independentemente de onde ou quando são acessados. \n",
    "\n",
    "Manter a consistência de dados é crucial para garantir a qualidade dos dados, o que, por sua vez, é essencial para a eficácia das análises de *data science* e para a tomada de decisões informadas.\n",
    "\n",
    "Por outro lado, a comparabilidade de dados é garantir que os dados recolhidos utilizando vários métodos ou fontes possam ser úteis. Isto frequentemente implica a padronização de formatos, tamanhos e unidades de dados, bem como a correção de quaisquer diferenças que possam surgir como resultado de vários metadados ou coleta de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistência de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A consistência dos dados garante que não haja informações conflitantes ou contraditórias e que os dados sejam precisos e confiáveis. As verificações de consistência comparam dados de datasets diferentes ou do mesmo dataset para encontrar discrepâncias e corrigi-las. \n",
    "\n",
    "Pontos-chave sobre a consistência de dados:\n",
    "- *Uniformidade*: Os dados devem ser consistentes em diferentes bancos de dados e sistemas. Por exemplo, se um cliente atualiza seu endereço em um sistema, essa atualização deve refletir em todos os outros sistemas que utilizam essa informação.\n",
    "- *Precisão*: Os dados devem ser corretos e livres de erros. Dados inconsistentes podem levar a análises incorretas e decisões equivocadas.\n",
    "- *Integridade*: A integridade dos dados é mantida quando não há conflitos ou discrepâncias entre os mesmos valores de dados em diferentes sistemas ou conjuntos de dados.\n",
    "- *Confiabilidade*: Dados consistentes são confiáveis e podem ser usados com segurança para análises e tomada de decisões.\n",
    "\n",
    "As discrepâncias entre os dados pode implicar a validação cruzada de dados, a confirmação das ligações entre as variáveis ​​e a detecção de quaisquer discrepâncias nos dados. \n",
    "\n",
    "As verificações de consistência de dados podem ser realizadas usando métodos como:\n",
    "- *Criação de perfil de dados (data profiling)* - oferece um conhecimento mais profundo das propriedades dos dados, fornecendo estatísticas resumidas e informações descritivas sobre os dados. \n",
    "- *Visualização de dados* - técnicas para dados, como gráficos de dispersão e caixa (box plotting), podem ser usadas para localizar discrepâncias potenciais. \n",
    "- *Estatística* - as correlações e a consistência dos dados podem ser validadas posteriormente usando técnicas de análise estatística, como análise de correlação ou hipótese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparabilidade de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A capacidade de comparar e analisar dados de várias fontes ou épocas é conhecida como *comparabilidade de dados*. O alinhamento de formatos de dados, a resolução de discrepâncias nas definições ou unidades de dados e a harmonização de variáveis ​​entre datasets são todos necessários para alcançar a comparabilidade dos dados. \n",
    "\n",
    "Pontos-chave sobre a comparabilidade de dados:\n",
    "\n",
    "- *Normalização de medidas*: - Os dados devem ser coletados e registrados usando as mesmas unidades de medida e critérios. Isso garante que as comparações sejam válidas e consistentes.\n",
    "- *Padronização*: A aplicação de padrões comuns para a coleta e processamento de dados é crucial. Isso inclui o uso de formatos de dados uniformes, nomenclatura consistente e metodologias de análise padronizadas.\n",
    "- *Contexto temporal*: Para que os dados sejam comparáveis ao longo do tempo, é importante que sejam coletados em intervalos regulares e que as condições de coleta permaneçam constantes.\n",
    "- *Qualidade dos dados*: Dados de alta qualidade, que são precisos, completos e livres de erros, são essenciais para garantir a comparabilidade. Dados inconsistentes ou de baixa qualidade podem levar a conclusões errôneas.\n",
    "- *Metadados*: A documentação detalhada dos dados, incluindo informações sobre como e quando foram coletados, é fundamental para permitir comparações precisas.\n",
    "\n",
    "A normalização e a padronização são essenciais para alcançar a comparabilidade dos dados. Os datasets podem ser combinados e comparados com facilidade se a formatação,  as regras de nomenclatura e tipos de dados forem semelhantes.\n",
    "Além disso, resolver inconsistências de medição ou contabilizar o crescimento exagerado de dados pode ajudar a garantir que os dados sejam semelhantes ao longo do tempo.\n",
    "\n",
    "A comparabilidade de dados é essencial para análises robustas e para a tomada de decisões informadas, especialmente em projetos de *data science* que envolvem grandes volumes de dados de múltiplas fontes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "####  Fudamentos para Ciêcia de Dados &copy; Copyright 2021, 2022, 2024 - Sergio Serra & Jorge Zavaleta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
