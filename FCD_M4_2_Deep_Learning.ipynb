{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPGI_UFRJ](imagens/ppgi-ufrj.png)\n",
    "# Fundamentos de Ciência de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[![DOI](https://zenodo.org/badge/335308405.svg)](https://zenodo.org/badge/latestdoi/335308405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PPGI/UFRJ 2020.3\n",
    "## Prof Sergio Serra e Jorge Zavaleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Módulo 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.x\n",
    "\n",
    "Um **tensor** é um objeto matemático e uma generalização de escalares, vetores e matrizes. Um tensor pode ser representado como uma matriz multidimensional.\n",
    "\n",
    "Um tensor de ordem zero (rank) é um escalar. Um vetor / matriz é um tensor de ordem 1, enquanto uma matriz é um tensor de ordem 2. Em consequência, um tensor pode ser considerado uma matriz n-dimensional.\n",
    "\n",
    "Exemplos de tensores:\n",
    "- 7 : é um tensor de ordem 0. Escalar com shape [].\n",
    "- [3.,5, 13]: Tensor de ordem 1. É um vetor com shape [3]\n",
    "- [[10,4,6],[3,4,5]]: É um tensor de ordem 2. É uma matriz com shpe [2,3]\n",
    "- [[[17,5,1]],[[9,6,3]]]: É um tensor de ordem 3 com shape [2,1,3]\n",
    "\n",
    "O tensorflow 2.x:\n",
    "\n",
    "- TensorFlow é uma biblioteca Python de código aberto para computação numérica criada para facilitar o aprendizado de máquina e a resolução de problemas de aprendizado profundo. \n",
    "- TensorFlow reúne módulos de aprendizado de máquina, módulos de aprendizado profundo e algoritmos associados em um ambiente de programação comum. \n",
    "- TensorFlow 2.x é a versão mais atual do software. \n",
    "- Usamos a designação 2.x porque o software está mudando muito rapidamente, atualemente está na versão 2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook apresenta conceitos básicos de aprendizagem profunda. O TensorFlow 2.10.0, pode usar o serviço de nuvem do Google, e o Google Drive Interactive para concretizar os conceitos com exemplos de codificação em Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo Computacional e Session v1.0 \n",
    "\n",
    "O **tensorflow** tem dois programas básico que formam parte do kernel e executam duas ações principais:\n",
    "- Construir um grafo computacional na fase inicial (fase de construção)\n",
    "- Rodar ou executar o grafo computacional na fase de execução.\n",
    "\n",
    "Um **grafo computacional** é uma série de operações do TensorFlow organizadas em nós de um gráfo.\n",
    "\n",
    "Em **TensorFlow**, se pode configurar um grafo (um grafo padrão). Em seguida, se precisa criar variáveis, marcadores de posição e  valores constantes e, em seguida, se cria a sessão e inicializa as variáveis. Finalmente, se alimenta com os dados os marcadores de posição, de modo a invocar qualquer ação.\n",
    "\n",
    "Para finalmente avaliar os nós, se deve executar o grafo computacional em uma **sessão**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observação\n",
    "\n",
    "O TensorFlow 2.x simplificou bastante o desenvolvimento de modelos em comparação com o TensorFlow 1.x. No TensorFlow 2.x, o uso explícito de gráficos computacionais e sessões foi substituído por uma abordagem mais intuitiva e direta:\n",
    "- *Grafo Computacional*: No TensorFlow 2.x, não é necessário definir gráficos explicitamente. Ele usa a execução ansiosa por padrão, o que significa que as operações são avaliadas imediatamente.\n",
    "- *Sessão*: As sessões, que eram necessárias para executar o grafo no TensorFlow 1.x, não são mais necessárias no TensorFlow 2.x.\n",
    "\n",
    "Essa alteração faz com que o TensorFlow 2.x se assemelhe mais ao código Python comum, tornando o desenvolvimento e a depuração muito mais fáceis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "**Keras** é uma biblioteca de rede neural de código aberto escrita em Python que pode ser executada em TensorFlow, Microsoft Cognitive Toolkit (CNTK), Theano, R e PlaidML (Keras é uma API) \n",
    "- Foi projetada para permitir experimentação rápida em redes neurais profundas.\n",
    "- Keras é simples e fácil de usar, modular e extensivel, flexível e poderosa. \n",
    "- Keras é vista como a biblioteca de aprendizagem profunda preferida pelos iniciantes.\n",
    "\n",
    "A estrurura de dados básica do Keras são as **camadas** (layer) e os **modelos** (models). Maiores detalhes em [Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar as bilbliotecas\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "#from tensorflow.estimator import LinearRegressor\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "from numpy import sqrt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar a versão do Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. importar as bibliotecas\n",
    "import tensorflow as tf\n",
    "#\n",
    "print('tf:',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear Usando TensorFlow e Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Preços das casas em Boston. Dataset descontinuado, a versão utilizada neste exemplo é uma versão modificada de *Harrison, D. and Rubinfeld, D. L. 'Hedonic prices and the demand for clean ar', J. Environ Economics & Management, vol. 5, 81-102, 1978*\n",
    "\n",
    "Este dataset poder ser utilizado para a previsão da taxa média de ocupação das casas em Boston.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho do dataset\n",
    "path = 'data/'\n",
    "# read dataset\n",
    "def read_dataset(path,dataset_name):\n",
    "    ds = pd.read_csv(path+dataset_name,sep=',',encoding='utf-8',low_memory=False,on_bad_lines='skip')#, index_col=0)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a CSV file\n",
    "dataset_name = 'Boston_housing.csv'\n",
    "houses_data = read_dataset(path,dataset_name)\n",
    "houses_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape\n",
    "houses_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target column (valor médio de las casas) MEDV = PRICE\n",
    "target_column = houses_data.MEDV\n",
    "print(target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análises exploratória de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in houses_data.items():\n",
    "    sns.boxplot(y=k, data=houses_data, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colunas como CRIM, ZN, RM e B parecem ter valores discrepantes (*outliers*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(houses_data.corr().abs(),annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da matriz de correlação, vemos que TAX e RAD são características altamente correlacionadas. As colunas LSTAT, RM, PTRAIO têm uma pontuação de correlação acima de 0,5 com MEDV, o que é uma boa indicação de uso como preditores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_data['MEDV'] = target_column.astype(np.float32)\n",
    "houses_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando as relações entre duas variáveis (distribuições bivariantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. relações entre variaveis\n",
    "sns.pairplot(houses_data, diag_kind=\"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medir o grau de correlação entre duas variáveis. Pode indicar se elas tendem a varaiar juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação\n",
    "correlation_data = houses_data.corr()\n",
    "correlation_data.style.background_gradient(cmap='coolwarm', axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estatística descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatistica descritiva\n",
    "stats = houses_data.describe()\n",
    "houses_stats = stats.transpose()\n",
    "houses_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionar os conjuntos de treinamento (X) e objetivo (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar colunas de dados requeridas (X_data) \n",
    "X_data = houses_data[[i for i in houses_data.columns if i not in ['MEDV']]]\n",
    "# selecionar o conjunto objetivo (Y_data)\n",
    "Y_data = houses_data[['MEDV']] #MEDV = PRICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar os conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento e teste (80% e 20%)\n",
    "X_train , X_test ,y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. de filas no Treinamento: ', X_train.shape[0])\n",
    "print('No. de filas no Teste: ', X_test.shape[0])\n",
    "print()\n",
    "print('No. de colunas no Treinamento: ', X_train.shape[1])\n",
    "print('No. de colunas no Teste: ', X_test.shape[1])\n",
    "print()\n",
    "print('No. de filas nas rótulos de Treinamento: ', y_train.shape[0])\n",
    "print('No. de filas nos rótulos de Teste: ', y_test.shape[0])\n",
    "print()\n",
    "print('No. de colunas nos rótulos de Treinamento: ', y_train.shape[1])\n",
    "print('No. de colunas nos rótulos de Teste: ', y_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizado os dados\n",
    "def norm(x):\n",
    "    stats = x.describe()\n",
    "    stats = stats.transpose()\n",
    "    return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar os dados de trenamento\n",
    "normed_train_features = norm(X_train)\n",
    "#print(normed_train_features)\n",
    "# normalizar os dados de teste\n",
    "normed_test_features = norm(X_test)\n",
    "#print(normed_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline do modelo do tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruir o pipeline para modelo do TensorFlow\n",
    "def feed_input(features_dataframe, target_dataframe,num_of_epochs=10, shuffle=True, batch_size=32):\n",
    "    #\n",
    "    def input_feed_function():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(features_dataframe), target_dataframe))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(2000)\n",
    "        dataset = dataset.batch(batch_size).repeat(num_of_epochs)\n",
    "        return dataset\n",
    "    #\n",
    "    return input_feed_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando a função definida no pipeline\n",
    "train_feed_input = feed_input(normed_train_features,y_train)\n",
    "train_feed_input_testing = feed_input(normed_train_features,y_train, num_of_epochs=1, shuffle=False)\n",
    "test_feed_input = feed_input(normed_test_features,y_test, num_of_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelando o treiamento\n",
    "feature_columns_numeric = [tf.feature_column.numeric_column(m) for m in X_train.columns]\n",
    "# modelo\n",
    "linear_model = tf.estimator.LinearRegressor(feature_columns=feature_columns_numeric, \n",
    "                               optimizer='RMSProp',\n",
    "                               model_dir='D:\\\\tf')\n",
    "#\n",
    "linear_model.train(train_feed_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predições\n",
    "train_predictions = linear_model.predict(train_feed_input_testing)\n",
    "test_predictions = linear_model.predict(test_feed_input)\n",
    "train_predictions_series = pd.Series([p['predictions'][0] for p in train_predictions])\n",
    "test_predictions_series = pd.Series([p['predictions'][0] for p in test_predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalia o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avalia o modelo\n",
    "train_predictions_df = pd.DataFrame(train_predictions_series, columns=['predictions'])\n",
    "test_predictions_df = pd.DataFrame(test_predictions_series, columns=['predictions'])\n",
    "#\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "train_predictions_df.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "test_predictions_df.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "train_labels_with_predictions_df = pd.concat([y_train, train_predictions_df], axis=1)\n",
    "test_labels_with_predictions_df = pd.concat([y_test,test_predictions_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valida o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validação\n",
    "def calculate_errors_and_r2(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mse = rmse*2\n",
    "    r2 = round(r2_score(y_true, y_pred)*100,0)\n",
    "    return mse, rmse, r2\n",
    "#\n",
    "train_mean_squared_error, train_root_mean_squared_error,train_r2_score_percentage = calculate_errors_and_r2(y_train, train_predictions_series)\n",
    "#\n",
    "test_mean_squared_error, test_root_mean_squared_error,test_r2_score_percentage = calculate_errors_and_r2(y_test, test_predictions_series)\n",
    "#\n",
    "print('Dados de Treinamento - Mean Squared Error = ', train_mean_squared_error)\n",
    "print('Dados de Treinamento - Root Mean Squared Error = ', train_root_mean_squared_error)\n",
    "print('Dados de Treinamento - R2 = ', train_r2_score_percentage)\n",
    "print('Dado de Teste - Mean Squared Error = ', test_mean_squared_error)\n",
    "print('Dados de Teste - Root Mean Squared Error = ', test_root_mean_squared_error)\n",
    "print('Dados de Teste - R2 = ', test_r2_score_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outra forma de fazer o mesmo exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar os conjuntos em dados e objetivo (data e target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output columns\n",
    "X, y = houses_data.values[:, :-1], houses_data.values[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir os conjuntos para treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar o número de características de entrada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "#model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "history = model.fit(X_train, y_train, epochs=167, batch_size=32, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "#error = model.evaluate(X_test, y_test, verbose=0)\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))\n",
    "#print('rmse:%.3f'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer previsões com o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "#yhat = model.predict([row])\n",
    "yhat = model.predict([row])\n",
    "print(' Valor previsto de uma casa em Boston: %.3f' % yhat[0][0],'dolares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfica as curvas de aprendizagem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.title('Curvas de Aprendizagem')\n",
    "plt.xlabel('Epocas (iterações)')\n",
    "plt.ylabel('Entropia Cruzada')\n",
    "plt.plot(history.history['loss'], label='treinamento')\n",
    "plt.plot(history.history['val_loss'], label='validação')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contsruir uma Rede Neural\n",
    "Dataset **MNIST dataset** [Dataset](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recohecimento de dígitos manuscritos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset usando o Keras\n",
    "(training_images, training_labels), (test_images, test_labels) = ks.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando alguns dados\n",
    "print('Imagenes de Treinamento - Dataset Shape: {}'.format(training_images.shape))\n",
    "print('No. de imagens de Treinamento - Dataset Labels: {}'.format(len(training_labels)))\n",
    "print('Imagens de teste - Dataset Shape: {}'.format(test_images.shape))\n",
    "print('No. de imagens de Teste - Dataset Labels: {}'.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deterinar o tamanho das características de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-escala (0-255) -> 0-1\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando keras para as diferentes camadas\n",
    "input_data_shape = (28, 28)                                # entrada de 28x28 pixels\n",
    "#\n",
    "hidden_activation_function = 'relu'                        # função de ativação - camada oculta\n",
    "output_activation_function = 'softmax'                     # função de ativação  - camada de saida\n",
    "#\n",
    "nn_model = ks.Sequential()                                 # modelo sequencial\n",
    "#\n",
    "nn_model.add(ks.layers.Flatten(input_shape=input_data_shape, name='Input_layer'))\n",
    "nn_model.add(ks.layers.Dense(32, activation=hidden_activation_function, name='Hidden_layer'))\n",
    "nn_model.add(ks.layers.Dense(10, activation=output_activation_function, name='Output_layer'))\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otimizando : SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "#\n",
    "optimizer = 'adam'\n",
    "loss_function = 'sparse_categorical_crossentropy'\n",
    "metric = ['accuracy']\n",
    "#\n",
    "nn_model.compile(optimizer=optimizer, loss=loss_function, metrics=metric)    # compilar\n",
    "#\n",
    "#nn_model.fit(training_images, training_labels, epochs=10)                    # ajustar ao modelo\n",
    "history = nn_model.fit(training_images, training_labels, epochs=10, batch_size=32, validation_split=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do treinamento\n",
    "training_loss, training_accuracy = nn_model.evaluate(training_images, training_labels)\n",
    "print('Acurácia dos dados de Treinamento {}'.format(round(float(training_accuracy),2)*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do teste\n",
    "test_loss, test_accuracy = nn_model.evaluate(test_images,test_labels)\n",
    "print('Acurácia dos dados de Teste {}'.format(round(float(test_accuracy),2)*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficar as curvas de aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.title('Curvas de aprendizagem')\n",
    "plt.xlabel('Epocas')\n",
    "plt.ylabel('Entropia cruzada categórica')\n",
    "plt.plot(history.history['loss'], label='trainamento')\n",
    "plt.plot(history.history['val_loss'], label='validação')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: ionosphere.csv\n",
    "\n",
    "O dataset da ionosfera original do repositório de aprendizado de máquina da UCI é um dataset de classificação binária com dimensionalidade 34. \n",
    "\n",
    "Há um atributo com valores todos zeros, que é descartado. Portanto, o número total de dimensões é 33. A classe \"ruim\" é considerada como classe de outliers e a classe \"boa\" como inliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a CSV file\n",
    "data_name = 'data/ionosphere.csv'\n",
    "ionos_data = pd.read_csv(data_name,header=None)\n",
    "ionos_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir em conjuntos de  entrada e saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output columns\n",
    "X, y = ionos_data.values[:, :-1], ionos_data.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferir os tipos de dados (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificar saídas (string) para inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "#y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar o número de entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define o modelo\n",
    "modelo = Sequential()\n",
    "modelo.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "modelo.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "modelo.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "y_pred = modelo.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, acc = modelo.evaluate(X_test, y_test, verbose=0)\n",
    "print('Acurácio do teste: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "valores = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = modelo.predict([valores])\n",
    "print('Valores previstos: {:.1f}%'.format(100*yhat[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.title('Curvas de aprendizagem ionosphere')\n",
    "plt.xlabel('Epocas')\n",
    "plt.ylabel('Entropia cruzada binária')\n",
    "plt.plot(y_pred.history['loss'], label='trainamento')\n",
    "plt.plot(y_pred.history['val_loss'], label='validação')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumo do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the model\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar o gráfico do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "# summarize the model\n",
    "tf.keras.utils.plot_model(modelo, 'images/modelo.png', show_shapes=True)\n",
    "#plot_model(modelo, 'images/modelo.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gravar o modelo 'h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "modelo.save('data/modelo.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ler o modelo gravado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modelo\n",
    "modelos = tf.keras.models.load_model('data/modelo.h5')\n",
    "# make a prediction\n",
    "val = [1,0,0.99539,-0.05889,0.85243,-0.02306,-0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "ym = modelos.predict([val])\n",
    "print(\"Previsto: {:.1f}%\".format(100*ym[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gravar modelo 'jolib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gravar arquivo jolib\n",
    "#arquivo_joblib = \"data/modelo_treinado_joblib.sav\"\n",
    "#joblib.dump(modelo, arquivo_joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo; Não funciona com keras e tensorflow!!!\n",
    "#modelo_joblib = joblib.load(arquivo_joblib)\n",
    "#result = carregando_modelo_joblib.score(previsores_test, classes_test)\n",
    "#val = [1,0,0.99539,-0.05889,0.85243,-0.02306,-0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "#resultado = modelo_joblib.predict([val])\n",
    "#print(f\"Percentual de Acertos {(resultado*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gravando usando Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gravar arquivo: funciona na versão python>3.6 e python <3.8\n",
    "#from pickle5 import pickle\n",
    "#\n",
    "#arquivo_pickle = \"data/modelo_treinado_pickle.sav\"\n",
    "#pickle.dump(modelo, open(arquivo_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# Carregando o modelo\n",
    "#carregando_modelo_pickle = pickle.load(open(arquivo_pickle, 'rb'))\n",
    "#\n",
    "# testando o modelo usando a base de dados de teste\n",
    "#val = [1,0,0.99539,-0.05889,0.85243,-0.02306,-0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "#resultado = modelo_joblib.predict([val])\n",
    "#result = carregando_modelo_pickle.score(previsores_test, classes_test)\n",
    "#print(f\"Percentual de Acertos {(resultado*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fundamentos para Ciência Dados &copy; Sergio Serra & Jorge Zavaleta, 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
